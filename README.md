# HuggingFace-DiffusionModels

##[HuggingFace Diffusion Models Class](https://github.com/darcula1993/diffusion-models-class-CN/)  

###Unit2: Fine-Tuning, Guidance and Conditioning  

Hands-On Notebook: 1. [Fine-tuning and Guidance](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb)  


Fine-Tuning: å¾®è°ƒï¼š  
Consideration 2: Our loss signal is extremely noisy, since we're only working with four examples at random noise levels for each step. This is not ideal for training. One fix is to use an extremely low learning rate to limit the size of the update each step. It would be even better if we could find some way to get the same benefit we would get from using a larger batch size without the memory requirements skyrocketing...  
Enter [gradient accumulation](https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html#:~:text=Simply%20speaking%2C%20gradient%20accumulation%20means,might%20find%20this%20tutorial%20useful.). If we call `loss.backward()` multiple times before running `optimizer.step()` and `optimizer.zero_grad()`, then PyTorch accumulates (sums) the gradients, effectively merging the signal from several batches to give a single (better) estimate which is then used to update the parameters. This results in fewer total updates being made, just like we'd see if we used a larger batch size. This is something many frameworks will handle for you (for example, [ğŸ¤— Accelerate makes this easy](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation) but it is nice to see it implemented from scratch since this is a useful technique for dealing with training under GPU memory constraints! As you can see from the code above (after the `# Gradient accumulation` comment) there really isn't much code needed.  
è€ƒè™‘å› ç´ 2ï¼š æˆ‘ä»¬çš„æŸå¤±å€¼æ›²çº¿ç®€ç›´åƒå™ªå£°ä¸€æ ·æ··ä¹±ï¼è¿™æ˜¯å› ä¸ºæ¯ä¸€æ¬¡è¿­ä»£æˆ‘ä»¬éƒ½åªç”¨äº†å››ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè€Œä¸”åŠ åˆ°å®ƒä»¬ä¸Šé¢çš„å™ªå£°æ°´å¹³è¿˜éƒ½æ˜¯éšæœºæŒ‘é€‰çš„ã€‚è¿™å¯¹äºè®­ç»ƒæ¥è®²å¹¶ä¸ç†æƒ³ã€‚ä¸€ç§å¼¥è¡¥çš„æªæ–½æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªéå¸¸å°çš„å­¦ä¹ ç‡ï¼Œé™åˆ¶æ¯æ¬¡æ›´æ–°çš„å¹…åº¦ã€‚ä½†æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•ï¼Œæ—¢èƒ½å¾—åˆ°å’Œä½¿ç”¨æ›´å¤§çš„ batch size ä¸€æ ·çš„æ”¶ç›Šï¼Œåˆä¸éœ€è¦è®©æˆ‘ä»¬çš„å†…å­˜çˆ†æ‰ã€‚  
ç‚¹å‡»è¿™é‡Œçœ‹çœ‹ï¼š[gradient accumulation](https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html#:~:text=Simply%20speaking%2C%20gradient%20accumulation%20means,might%20find%20this%20tutorial%20useful.)ã€‚å¦‚æœæˆ‘ä»¬å¤šè¿è¡Œå‡ æ¬¡`loss.backward()`åå†è°ƒç”¨`optimizer.step()`å’Œ`optimizer.zero_grad()`ï¼ŒPyTorch å°±ä¼šæŠŠæ¢¯åº¦ç´¯ç§¯ï¼ˆåŠ å’Œï¼‰èµ·æ¥ï¼Œè¿™æ ·å¤šä¸ªæ‰¹æ¬¡çš„æ•°æ®äº§ç”Ÿçš„æ›´æ–°ä¿¡å·å°±ä¼šè¢«é«˜æ•ˆåœ°èåˆåœ¨ä¸€èµ·ï¼Œäº§å‡ºä¸€ä¸ªå•ç‹¬çš„ï¼ˆæ›´å¥½çš„ï¼‰æ¢¯åº¦ä¼°è®¡ç”¨äºå‚æ•°æ›´æ–°ã€‚è¿™æ ·åšä¼šå‡å°‘å‚æ•°æ›´æ–°çš„æ€»æ¬¡æ•°ï¼Œå°±æ­£å¦‚æˆ‘ä»¬ä½¿ç”¨æ›´å¤§çš„ batch size æ—¶å¸Œæœ›çœ‹åˆ°çš„ä¸€æ ·ã€‚æ¢¯åº¦ç´¯ç§¯æ˜¯ä¸€ä¸ªå¾ˆå¤šæ¡†æ¶éƒ½ä¼šæ›¿ä½ åšçš„äº‹æƒ…ï¼ˆæ¯”å¦‚è¿™é‡Œï¼š[ğŸ¤— Accelerate makes this easy](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)ï¼‰ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä»å¤´å®ç°ä¸€éä¹ŸæŒºå¥½çš„ï¼Œå› ä¸ºè¿™å¯¹ä½ åœ¨ GPU å†…å­˜å—é™æ—¶è®­ç»ƒæ¨¡å‹éå¸¸æœ‰å¸®åŠ©ã€‚æ­£å¦‚ä½ åœ¨ä¸Šé¢ä»£ç ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼ˆåœ¨æ³¨é‡Š `# Gradient accumulation` åï¼‰ï¼Œå…¶å®ä¹Ÿä¸éœ€è¦ä½ å†™å¾ˆå¤šä»£ç ã€‚  

è§£å†³æ–¹æ³•ï¼šä½¿ç”¨é€šä¹‰åƒé—®å¸®æˆ‘æŒ‰è¦æ±‚ä¿®æ”¹ä»£ç ã€‚  
```Python
num_epochs = 2  # @param
lr = 1e-5  # 2param
grad_accumulation_steps = 2  # @param

optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)

losses = []

for epoch in range(num_epochs):
    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
        clean_images = batch["images"].to(device)
        # Sample noise to add to the images
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]

        # Sample a random timestep for each image
        timesteps = torch.randint(
            0,
            image_pipe.scheduler.num_train_timesteps,
            (bs,),
            device=clean_images.device,
        ).long()

        # Add noise to the clean images according to the noise magnitude at each timestep
        # (this is the forward diffusion process)
        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)

        # Get the model prediction for the noise
        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]

        # Compare the prediction with the actual noise:
        loss = F.mse_loss(
            noise_pred, noise
        )  # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)

        # Store for later plotting
        losses.append(loss.item())

        # Update the model parameters with the optimizer based on this loss
        loss.backward(loss)

        # Gradient accumulation:
        if (step + 1) % grad_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

    print(
        f"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}"
    )

# Plot the loss curve:
plt.plot(losses)
```
```
æé—®ï¼š
æˆ‘æ­£åœ¨è¿›è¡Œå¯¹é¢„è®­ç»ƒè¿‡çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¸Šæ˜¯æˆ‘çš„è®­ç»ƒå¾ªç¯éƒ¨åˆ†çš„ä»£ç ã€‚ä»£ç è¿è¡Œåï¼ŒæŸå¤±å€¼æ›²çº¿ç®€ç›´åƒå™ªå£°ä¸€æ ·æ··ä¹±ã€‚è¿™æ˜¯å› ä¸ºæ¯ä¸€æ¬¡è¿­ä»£æˆ‘ä»¬éƒ½åªç”¨äº†å››ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè€Œä¸”åŠ åˆ°å®ƒä»¬ä¸Šé¢çš„å™ªå£°æ°´å¹³è¿˜éƒ½æ˜¯éšæœºæŒ‘é€‰çš„ã€‚è¿™å¯¹äºè®­ç»ƒæ¥è®²å¹¶ä¸ç†æƒ³ã€‚å¦‚æœæˆ‘ä»¬å¤šè¿è¡Œå‡ æ¬¡loss.backward()åå†è°ƒç”¨optimizer.step()å’Œoptimizer.zero_grad()ï¼ŒPyTorch å°±ä¼šæŠŠæ¢¯åº¦ç´¯ç§¯ï¼ˆåŠ å’Œï¼‰èµ·æ¥ï¼Œè¿™æ ·å¤šä¸ªæ‰¹æ¬¡çš„æ•°æ®äº§ç”Ÿçš„æ›´æ–°ä¿¡å·å°±ä¼šè¢«é«˜æ•ˆåœ°èåˆåœ¨ä¸€èµ·ï¼Œäº§å‡ºä¸€ä¸ªå•ç‹¬çš„ï¼ˆæ›´å¥½çš„ï¼‰æ¢¯åº¦ä¼°è®¡ç”¨äºå‚æ•°æ›´æ–°ã€‚  

è¯·æ ¹æ®ä»¥ä¸Šæ–‡å­—æè¿°ï¼Œå¸®æˆ‘ä¿®æ”¹ä»£ç ã€‚
```
```
å›ç­”ï¼š
ä¸ºäº†æ”¹è¿›ä½ çš„è®­ç»ƒå¾ªç¯ï¼Œä½¿å¾—æ¢¯åº¦ç´¯ç§¯å¯ä»¥æ›´æœ‰æ•ˆåœ°å·¥ä½œï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿åœ¨æ¯ä¸ªepochä¸­å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå®Œæ•´çš„éå†ï¼Œå¹¶ä¸”åœ¨ç´¯ç§¯äº†grad_accumulation_stepsä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦ä¹‹åæ‰æ›´æ–°æ¨¡å‹å‚æ•°ã€‚è¿™æ ·å¯ä»¥å¸®åŠ©å¹³æ»‘æŸå¤±æ›²çº¿ï¼Œå› ä¸ºæ¯æ¬¡å‚æ•°æ›´æ–°éƒ½ä¼šåŸºäºæ›´å¤šçš„æ ·æœ¬ã€‚

è¿™é‡Œæ˜¯ä¿®æ”¹åçš„ä»£ç æ®µï¼š
```
```Python
# å‡è®¾image_pipe, train_dataloader, device, Fç­‰å˜é‡å·²ç»å®šä¹‰å¥½
num_epochs = 2  # @param
lr = 1e-5  # @param
grad_accumulation_steps = 2  # @param

# åˆå§‹åŒ–ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)

losses = []

for epoch in range(num_epochs):
    epoch_loss = 0.0  # ç”¨äºè®¡ç®—æ¯ä¸ªepochçš„å¹³å‡æŸå¤±
    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):
        clean_images = batch["images"].to(device)
        
        # æ·»åŠ å™ªå£°åˆ°å›¾ç‰‡
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]
        timesteps = torch.randint(
            0,
            image_pipe.scheduler.num_train_timesteps,
            (bs,),
            device=clean_images.device,
        ).long()
        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)
        
        # é¢„æµ‹å™ªå£°
        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(noise_pred, noise)
        epoch_loss += loss.item()  # ç´¯ç§¯æŸå¤±
        
        # åå‘ä¼ æ’­ï¼Œä½†ä¸ç«‹å³æ‰§è¡Œæ¢¯åº¦ä¸‹é™
        loss.backward()

        # å¦‚æœç´¯ç§¯äº†è¶³å¤Ÿçš„æ¢¯åº¦ï¼Œåˆ™æ›´æ–°æƒé‡å¹¶æ¸…ç©ºæ¢¯åº¦
        if (step + 1) % grad_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
    
    # å¯¹äºæœ€åä¸€ä¸ªbatchï¼Œå¦‚æœå®ƒæ²¡æœ‰è¢«ç´¯ç§¯åˆ°grad_accumulation_stepsæ¬¡ï¼Œä¹Ÿéœ€è¦æ›´æ–°æƒé‡
    if (step + 1) % grad_accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()
    
    # è®°å½•æ¯ä¸ªepochçš„å¹³å‡æŸå¤±
    avg_epoch_loss = epoch_loss / len(train_dataloader)
    losses.append(avg_epoch_loss)
    print(f"Epoch {epoch} average loss: {avg_epoch_loss}")

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.plot(losses)
plt.xlabel('Batches')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.show()
```
```
è¿™æ®µä»£ç ä¸­çš„ä¸»è¦å˜åŒ–åŒ…æ‹¬ï¼š

åœ¨æ¯ä¸ªepochç»“æŸæ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œé¢å¤–çš„optimizer.step()å’Œoptimizer.zero_grad()è°ƒç”¨ï¼Œä»¥å¤„ç†é‚£äº›æ²¡æœ‰è¾¾åˆ°grad_accumulation_stepsæ­¥æ•°å°±è¢«epochç»“æŸæ‰“æ–­çš„æƒ…å†µã€‚
epoch_lossç”¨æ¥å­˜å‚¨å½“å‰epochçš„æ‰€æœ‰æŸå¤±ä¹‹å’Œï¼Œç„¶ååœ¨æ¯ä¸ªepochç»“æŸæ—¶è®¡ç®—å¹³å‡æŸå¤±å¹¶æ‰“å°å‡ºæ¥ã€‚
lossesåˆ—è¡¨ç°åœ¨è®°å½•çš„æ˜¯æ¯ä¸ªepochçš„å¹³å‡æŸå¤±ï¼Œè€Œä¸æ˜¯æ¯ä¸€æ­¥çš„æŸå¤±ï¼Œè¿™æœ‰åŠ©äºç»˜åˆ¶æ›´åŠ å¹³æ»‘çš„æŸå¤±æ›²çº¿ã€‚
```
